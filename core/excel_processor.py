"""
Excelæ™ºèƒ½å¤„ç†æ¨¡å—
æ”¯æŒï¼š
1. è‡ªåŠ¨è·³è¿‡æ— æ•ˆè¡Œï¼ˆæ³¨é‡Šã€æ ‡é¢˜ç­‰ï¼‰
2. å•è¡¨å¤´/å¤šè¡¨å¤´è‡ªåŠ¨è¯†åˆ«
3. å¯é€‰è°ƒç”¨LLMè¿›è¡Œæ™ºèƒ½åˆ†æ
4. åˆå¹¶å•å…ƒæ ¼å¤„ç†
5. åˆ—ç»“æ„å…ƒæ•°æ®ç”Ÿæˆ
"""

import pandas as pd
import json
import re
import os
import requests
import logging
from openpyxl import load_workbook
from typing import Tuple, List, Dict, Optional, Any
from collections import defaultdict
from dataclasses import dataclass, asdict, field
from pathlib import Path

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

# å¯¼å…¥é…ç½®ï¼ˆé¿å…å¾ªç¯å¯¼å…¥ï¼Œä½¿ç”¨å»¶è¿Ÿå¯¼å…¥ï¼‰

from .config import EXCEL_LLM_API_KEY, EXCEL_LLM_BASE_URL, EXCEL_LLM_MODEL, EXCEL_MAX_ROWS_PREVIEW, EXCEL_MAX_COLS_PREVIEW



@dataclass
class HeaderAnalysis:
    """è¡¨å¤´åˆ†æç»“æœ"""
    skip_rows: int          # éœ€è¦è·³è¿‡çš„æ— æ•ˆè¡Œæ•°
    header_rows: int        # è¡¨å¤´å ç”¨çš„è¡Œæ•°
    header_type: str        # 'single' æˆ– 'multi'
    data_start_row: int     # æ•°æ®å¼€å§‹è¡Œï¼ˆ1-indexedï¼‰
    confidence: str         # ç½®ä¿¡åº¦: high/medium/low
    reason: str             # åˆ†æåŸå› è¯´æ˜
    valid_cols: Optional[List[int]] = None  # æœ‰æ•ˆåˆ—çš„ç´¢å¼•åˆ—è¡¨ï¼ˆ1-indexedï¼‰ï¼ŒNoneè¡¨ç¤ºæ‰€æœ‰åˆ—éƒ½æœ‰æ•ˆ
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        result = asdict(self)
        if result.get('valid_cols') is None:
            result['valid_cols'] = None
        return result


@dataclass
class ExcelProcessResult:
    """Excelå¤„ç†ç»“æœ"""
    success: bool
    header_analysis: Optional[HeaderAnalysis]
    processed_file_path: Optional[str]      # å¤„ç†åçš„CSVæ–‡ä»¶è·¯å¾„
    metadata_file_path: Optional[str]       # å…ƒæ•°æ®JSONæ–‡ä»¶è·¯å¾„
    column_names: List[str]                 # åˆ—ååˆ—è¡¨
    column_metadata: Dict[str, Dict]        # åˆ—ç»“æ„å…ƒæ•°æ®
    row_count: int                          # æ•°æ®è¡Œæ•°
    error_message: Optional[str]            # é”™è¯¯ä¿¡æ¯
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "success": self.success,
            "header_analysis": self.header_analysis.to_dict() if self.header_analysis else None,
            "processed_file_path": self.processed_file_path,
            "metadata_file_path": self.metadata_file_path,
            "column_names": self.column_names,
            "column_metadata": self.column_metadata,
            "row_count": self.row_count,
            "error_message": self.error_message
        }


class SmartHeaderProcessor:
    """æ™ºèƒ½è¡¨å¤´å¤„ç†å™¨"""
    
    def __init__(self, filepath: str, sheet_name: str = None):
        self.filepath = filepath
        self.sheet_name = sheet_name
        self.wb = load_workbook(filepath, data_only=True)
        self.ws = self.wb[sheet_name] if sheet_name else self.wb.active
        self.merged_cells_map = self._build_merged_cells_map()
    
    def _build_merged_cells_map(self) -> Dict[Tuple[int, int], str]:
        """æ„å»ºåˆå¹¶å•å…ƒæ ¼æ˜ å°„"""
        merged_map = {}
        for merged_range in self.ws.merged_cells.ranges:
            min_row, min_col = merged_range.min_row, merged_range.min_col
            value = self.ws.cell(min_row, min_col).value
            for row in range(merged_range.min_row, merged_range.max_row + 1):
                for col in range(merged_range.min_col, merged_range.max_col + 1):
                    merged_map[(row, col)] = value
        return merged_map
    
    def get_cell_value(self, row: int, col: int) -> Any:
        """è·å–å•å…ƒæ ¼å€¼ï¼Œå¤„ç†åˆå¹¶å•å…ƒæ ¼"""
        if (row, col) in self.merged_cells_map:
            return self.merged_cells_map[(row, col)]
        return self.ws.cell(row, col).value
    
    def get_preview_data(self, max_rows: int = 15, max_cols: int = 10) -> List[List[Any]]:
        """è·å–é¢„è§ˆæ•°æ®ç”¨äºåˆ†æ"""
        actual_max_col = min(self.ws.max_column, max_cols)
        actual_max_row = min(self.ws.max_row, max_rows)
        
        data = []
        for row in range(1, actual_max_row + 1):
            row_data = []
            for col in range(1, actual_max_col + 1):
                value = self.get_cell_value(row, col)
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ä¾¿äºåˆ†æ
                if value is None:
                    row_data.append("")
                elif isinstance(value, (int, float)):
                    row_data.append(f"[æ•°å€¼:{value}]")
                else:
                    row_data.append(str(value)[:50])  # æˆªæ–­è¿‡é•¿å†…å®¹
            data.append(row_data)
        return data
    
    def get_merged_info(self) -> List[Dict]:
        """è·å–åˆå¹¶å•å…ƒæ ¼ä¿¡æ¯"""
        merged_info = []
        for merged_range in self.ws.merged_cells.ranges:
            if merged_range.min_row <= 10:  # åªå…³æ³¨å‰10è¡Œ
                merged_info.append({
                    'range': str(merged_range),
                    'rows': f"{merged_range.min_row}-{merged_range.max_row}",
                    'cols': f"{merged_range.min_col}-{merged_range.max_col}",
                    'value': str(self.ws.cell(merged_range.min_row, merged_range.min_col).value)[:30]
                })
        return merged_info
    
    def analyze_with_llm(self, 
                         llm_api_key: Optional[str] = None,
                         llm_base_url: Optional[str] = None,
                         llm_model: Optional[str] = None,
                         preview_max_rows: Optional[int] = None,
                         preview_max_cols: Optional[int] = None) -> HeaderAnalysis:
        """
        ä½¿ç”¨LLMç›´æ¥åˆ†æExcelè¡¨å¤´ç»“æ„ï¼ˆåŒ…å«è¡Œæ£€æµ‹å’Œåˆ—æ£€æµ‹ï¼‰
        
        å‚æ•°:
            llm_api_key: LLM APIå¯†é’¥ï¼ˆå¿…å¡«ï¼‰
            llm_base_url: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼‰
            llm_model: LLMæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
            preview_max_rows: é¢„è§ˆæœ€å¤§è¡Œæ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
            preview_max_cols: é¢„è§ˆæœ€å¤§åˆ—æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
        
        è¿”å›:
            HeaderAnalysis åˆ†æç»“æœ
        
        å¼‚å¸¸:
            å¦‚æœLLM API Keyæœªé…ç½®ï¼ŒæŠ›å‡ºValueError
        """
        # æ£€æŸ¥API Key
        api_key = llm_api_key if llm_api_key is not None else EXCEL_LLM_API_KEY
        if not api_key:
            raise ValueError("LLM API Key æœªé…ç½®ï¼ŒLLMåˆ†ææ˜¯å¿…éœ€çš„")
        
        # ä½¿ç”¨ä¼ å…¥çš„å‚æ•°æˆ–ä»é…ç½®è¯»å–é»˜è®¤å€¼
        max_rows = preview_max_rows if preview_max_rows is not None else EXCEL_MAX_ROWS_PREVIEW
        max_cols = preview_max_cols if preview_max_cols is not None else EXCEL_MAX_COLS_PREVIEW
        
        preview_data = self.get_preview_data(max_rows=max_rows, max_cols=max_cols)
        merged_info = self.get_merged_info()
        
        # è·å–åˆ—æ•°ä¿¡æ¯
        max_col = self.ws.max_column
        
        # æ„å»ºåˆ†ææç¤ºè¯ï¼ˆåŒ…å«è¡Œå’Œåˆ—æ£€æµ‹ï¼‰
        prompt = self._build_analysis_prompt(preview_data, merged_info, max_col)
        
        # è°ƒç”¨LLMï¼ˆä½¿ç”¨ä¼ å…¥çš„é…ç½®æˆ–ä»å…¨å±€é…ç½®è¯»å–ï¼‰
        result = self._call_llm(prompt, llm_api_key, llm_base_url, llm_model)
        
        if not result:
            raise ValueError("LLMè°ƒç”¨å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
        
        # è§£æLLMåˆ†æç»“æœï¼ˆåŒ…å«è¡Œå’Œåˆ—æ£€æµ‹ï¼‰
        analysis = self._parse_analysis_response(result)
        
        return analysis
    
    def _build_analysis_prompt(self, preview_data: List[List], merged_info: List[Dict], max_col: int) -> str:
        """æ„å»ºLLMåˆ†ææç¤ºè¯ï¼ˆåŒ…å«è¡Œæ£€æµ‹å’Œåˆ—æ£€æµ‹ï¼‰"""
        # æ ¼å¼åŒ–é¢„è§ˆæ•°æ®ä¸ºè¡¨æ ¼å½¢å¼
        table_str = "è¡Œå· | åˆ—1 | åˆ—2 | åˆ—3 | åˆ—4 | åˆ—5 | åˆ—6 | åˆ—7 | åˆ—8 | ...\n" + "-" * 80 + "\n"
        for i, row in enumerate(preview_data, 1):
            row_str = " | ".join(str(cell)[:15] for cell in row[:8])
            table_str += f"  {i:2d}  | {row_str}\n"
        
        # æ ¼å¼åŒ–åˆå¹¶å•å…ƒæ ¼ä¿¡æ¯
        merged_str = "æ— " if not merged_info else "\n".join(
            f"  - {m['range']}: '{m['value']}'" for m in merged_info[:10]
        )
        
        prompt = f"""è¯·åˆ†æä»¥ä¸‹Excelè¡¨æ ¼çš„ç»“æ„ï¼Œè¯†åˆ«è¡¨å¤´è¡Œã€æ•°æ®èµ·å§‹è¡Œå’Œæœ‰æ•ˆåˆ—ã€‚

ã€è¡¨æ ¼é¢„è§ˆã€‘ï¼ˆå‰{len(preview_data)}è¡Œï¼Œ[æ•°å€¼:xxx]è¡¨ç¤ºæ•°å€¼ç±»å‹ï¼Œç©ºå•å…ƒæ ¼æ˜¾ç¤ºä¸ºç©ºï¼‰
{table_str}

ã€åˆå¹¶å•å…ƒæ ¼ä¿¡æ¯ã€‘
{merged_str}

ã€è¡¨æ ¼ä¿¡æ¯ã€‘
- æ€»åˆ—æ•°: {max_col}
- æ€»è¡Œæ•°: {len(preview_data)}ï¼ˆé¢„è§ˆï¼‰

è¯·ä»”ç»†åˆ†æè¡¨æ ¼ç»“æ„ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{{
    "skip_rows": <éœ€è¦è·³è¿‡çš„æ— æ•ˆè¡Œæ•°ï¼ˆæ ‡é¢˜ã€æ³¨é‡Šç­‰ï¼‰ï¼Œä»ç¬¬1è¡Œå¼€å§‹è®¡æ•°>,
    "header_rows": <è¡¨å¤´å ç”¨çš„è¡Œæ•°>,
    "header_type": "<singleæˆ–multi>",
    "data_start_row": <æ•°æ®å¼€å§‹è¡Œï¼ˆ1-indexedï¼‰>,
    "valid_cols": [<æœ‰æ•ˆåˆ—çš„ç´¢å¼•åˆ—è¡¨ï¼Œ1-indexedï¼Œä¾‹å¦‚[1,2,3,5,7]è¡¨ç¤ºç¬¬1,2,3,5,7åˆ—æ˜¯æœ‰æ•ˆçš„>],
    "confidence": "<high/medium/low>",
    "reason": "<åˆ†æè¯´æ˜ï¼šè¯´æ˜å¦‚ä½•è¯†åˆ«è¡¨å¤´ã€æ•°æ®èµ·å§‹è¡Œå’Œæœ‰æ•ˆåˆ—>"
}}

åˆ†æè¦ç‚¹ï¼š
1. **è¡Œæ£€æµ‹**ï¼š
   - è¯†åˆ«éœ€è¦è·³è¿‡çš„æ— æ•ˆè¡Œï¼ˆé€šå¸¸æ˜¯æ ‡é¢˜ã€è¯´æ˜ç­‰ï¼Œéç©ºå•å…ƒæ ¼å¾ˆå°‘çš„è¡Œï¼‰
   - è¯†åˆ«è¡¨å¤´è¡Œï¼ˆé€šå¸¸åŒ…å«åˆ—åï¼Œå¯èƒ½æ˜¯å•è¡Œæˆ–å¤šè¡Œï¼‰
   - è¯†åˆ«æ•°æ®èµ·å§‹è¡Œï¼ˆç¬¬ä¸€è¡ŒåŒ…å«å®é™…æ•°æ®çš„è¡Œï¼Œé€šå¸¸åŒ…å«æ•°å€¼ï¼‰

2. **åˆ—æ£€æµ‹**ï¼š
   - è¯†åˆ«æœ‰æ•ˆåˆ—ï¼šè¡¨å¤´åŒºåŸŸæœ‰å†…å®¹æˆ–æ•°æ®åŒºåŸŸæœ‰æ•°å€¼æ•°æ®çš„åˆ—
   - è¿‡æ»¤æ— æ•ˆåˆ—ï¼šè¡¨å¤´åŒºåŸŸå®Œå…¨ä¸ºç©ºä¸”æ•°æ®åŒºåŸŸå®Œå…¨ä¸ºç©ºæˆ–æ²¡æœ‰æ•°å€¼æ•°æ®çš„åˆ—
   - valid_cols åº”è¯¥æ˜¯1-indexedçš„åˆ—ç´¢å¼•åˆ—è¡¨ï¼Œä¾‹å¦‚ [1,2,3,5,7] è¡¨ç¤ºç¬¬1,2,3,5,7åˆ—æ˜¯æœ‰æ•ˆçš„
   - å¦‚æœæ‰€æœ‰åˆ—éƒ½æœ‰æ•ˆï¼Œvalid_cols å¯ä»¥ä¸º null æˆ–åŒ…å«æ‰€æœ‰åˆ—ç´¢å¼•

3. **è¡¨å¤´ç±»å‹**ï¼š
   - single: å•è¡Œè¡¨å¤´
   - multi: å¤šè¡Œè¡¨å¤´ï¼ˆåˆå¹¶å•å…ƒæ ¼æˆ–åˆ†å±‚ç»“æ„ï¼‰

4. **æ³¨æ„äº‹é¡¹**ï¼š
   - skip_rows æ˜¯ä»ç¬¬1è¡Œå¼€å§‹éœ€è¦è·³è¿‡çš„è¡Œæ•°ï¼ˆä¾‹å¦‚skip_rows=2è¡¨ç¤ºè·³è¿‡ç¬¬1-2è¡Œï¼‰
   - data_start_row æ˜¯æ•°æ®å¼€å§‹çš„è¡Œå·ï¼ˆ1-indexedï¼‰
   - header_rows æ˜¯è¡¨å¤´å ç”¨çš„è¡Œæ•°
   - ç¡®ä¿ data_start_row = skip_rows + header_rows + 1
   - åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹"""
        
        return prompt
    
    def _call_llm(self, prompt: str, llm_api_key: Optional[str] = None, 
                  llm_base_url: Optional[str] = None, llm_model: Optional[str] = None) -> str:
        """è°ƒç”¨LLM APIï¼ˆæ”¯æŒOpenAIå…¼å®¹æ¥å£ï¼‰
        
        å‚æ•°:
            prompt: æç¤ºè¯
            llm_api_key: LLM APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä»é…ç½®è¯»å–ï¼‰
            llm_base_url: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä»é…ç½®è¯»å–ï¼‰
            llm_model: LLMæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™ä»é…ç½®è¯»å–ï¼‰
        """
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„å‚æ•°ï¼Œå¦åˆ™ä»é…ç½®è¯»å–
        api_key = llm_api_key if llm_api_key is not None else EXCEL_LLM_API_KEY
        base_url = llm_base_url if llm_base_url is not None else EXCEL_LLM_BASE_URL
        model = llm_model if llm_model is not None else EXCEL_LLM_MODEL
        
        logger.info("=" * 60)
        logger.info("ğŸ¤– è°ƒç”¨ LLM API è¿›è¡Œè¡¨å¤´åˆ†æï¼ˆåŒ…å«è¡Œæ£€æµ‹å’Œåˆ—æ£€æµ‹ï¼‰")
        logger.info(f"ğŸ”— EXCEL_LLM_BASE_URL: {base_url}")
        logger.info(f"ğŸ“Œ æ¨¡å‹: {model}")
        logger.info(f"ğŸ”‘ API Key: {'å·²é…ç½®' if api_key else 'æœªé…ç½®'}")
        
        if not api_key:
            raise ValueError("LLM API Key æœªé…ç½®ï¼ŒLLMåˆ†ææ˜¯å¿…éœ€çš„")
            
        url = base_url
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        
        payload = {
            "model": model,
            "max_tokens": 1000,  # å¢åŠ tokenæ•°é‡ä»¥æ”¯æŒåˆ—æ£€æµ‹ç»“æœ
            "messages": [{"role": "user", "content": prompt}]
        }
        
        logger.info(f"ğŸ“¡ å‘é€ LLM API è¯·æ±‚åˆ°: {url}")
        logger.info(f"ğŸ“ æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            result = response.json()
            llm_response = result['choices'][0]['message']['content']
            
            logger.info("âœ… LLM API è°ƒç”¨æˆåŠŸ")
            logger.info("=" * 60)
            logger.info("ğŸ“ LLM å“åº”å†…å®¹:")
            logger.info("=" * 60)
            logger.info(llm_response)
            logger.info("=" * 60)
            
            return llm_response
        except Exception as e:
            logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            logger.debug("å¼‚å¸¸è¯¦æƒ…:", exc_info=True)
            return None
    
    def _parse_analysis_response(self, response: str) -> HeaderAnalysis:
        """è§£æLLMåˆ†æç»“æœï¼ˆåŒ…å«è¡Œæ£€æµ‹å’Œåˆ—æ£€æµ‹ï¼‰"""
        if not response:
            raise ValueError("LLMå“åº”ä¸ºç©º")
        
        try:
            # æå–JSONéƒ¨åˆ†ï¼ˆæ”¯æŒåµŒå¥—JSONï¼‰
            # å…ˆå°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª } ä¹‹é—´çš„å†…å®¹
            start_idx = response.find('{')
            end_idx = response.rfind('}')
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                json_str = response[start_idx:end_idx + 1]
                data = json.loads(json_str)
            else:
                # å¦‚æœæ‰¾ä¸åˆ°å®Œæ•´çš„JSONï¼Œå°è¯•ç”¨æ­£åˆ™åŒ¹é…
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if not json_match:
                    raise ValueError("æœªæ‰¾åˆ°JSONæ ¼å¼çš„å“åº”")
                data = json.loads(json_match.group())
            
            # è§£æè¡Œæ£€æµ‹ç»“æœ
            skip_rows = data.get('skip_rows', 0)
            header_rows = data.get('header_rows', 1)
            header_type = data.get('header_type', 'single')
            data_start_row = data.get('data_start_row', skip_rows + header_rows + 1)
            confidence = data.get('confidence', 'medium')
            reason = data.get('reason', 'LLMåˆ†æç»“æœ')
            
            # è§£æåˆ—æ£€æµ‹ç»“æœ
            valid_cols = data.get('valid_cols', None)
            if valid_cols is None:
                # å¦‚æœä¸ºnullï¼Œè¡¨ç¤ºæ‰€æœ‰åˆ—éƒ½æœ‰æ•ˆ
                valid_cols = None
            elif isinstance(valid_cols, list):
                # ç¡®ä¿æ˜¯æ•´æ•°åˆ—è¡¨
                valid_cols = [int(col) for col in valid_cols if isinstance(col, (int, str))]
                # å¦‚æœåŒ…å«æ‰€æœ‰åˆ—ï¼Œè®¾ä¸ºNone
                max_col = self.ws.max_column
                if len(valid_cols) == max_col and set(valid_cols) == set(range(1, max_col + 1)):
                    valid_cols = None
            else:
                valid_cols = None
            
            # éªŒè¯æ•°æ®èµ·å§‹è¡Œçš„ä¸€è‡´æ€§
            if data_start_row != skip_rows + header_rows + 1:
                logger.warning(f"âš ï¸ æ•°æ®èµ·å§‹è¡Œä¸ä¸€è‡´ï¼ŒLLMè¿”å›: {data_start_row}ï¼Œè®¡ç®—å€¼: {skip_rows + header_rows + 1}ï¼Œä½¿ç”¨LLMè¿”å›çš„å€¼")
            
            return HeaderAnalysis(
                skip_rows=skip_rows,
                header_rows=max(1, header_rows),
                header_type=header_type,
                data_start_row=data_start_row,
                confidence=confidence,
                reason=f"LLMåˆ†æ: {reason}",
                valid_cols=valid_cols
            )
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            logger.error(f"è§£æLLMåˆ†æå“åº”å¤±è´¥: {e}")
            logger.error(f"å“åº”å†…å®¹: {response[:500]}")
            raise ValueError(f"è§£æLLMå“åº”å¤±è´¥: {e}")
    
    
    def extract_headers(self, analysis: HeaderAnalysis) -> Tuple[List[str], Dict[str, Dict]]:
        """
        æ ¹æ®åˆ†æç»“æœæå–è¡¨å¤´
        è¿”å›: (åˆ—ååˆ—è¡¨, åˆ—ç»“æ„å…ƒæ•°æ®)
        """
        max_col = self.ws.max_column
        header_start = analysis.skip_rows + 1
        header_end = analysis.skip_rows + analysis.header_rows
        
        # ç¡®å®šè¦å¤„ç†çš„åˆ—ï¼ˆå¦‚æœæŒ‡å®šäº†æœ‰æ•ˆåˆ—ï¼Œåªå¤„ç†æœ‰æ•ˆåˆ—ï¼‰
        cols_to_process = analysis.valid_cols if analysis.valid_cols is not None else list(range(1, max_col + 1))
        
        logger.info(f"ğŸ“‹ æå–è¡¨å¤´: å¤„ç† {len(cols_to_process)} åˆ—")
        
        column_metadata = {}
        
        if analysis.header_type == 'single':
            # å•è¡¨å¤´
            headers = []
            for col in cols_to_process:
                value = self.get_cell_value(header_start, col)
                col_name = str(value) if value else f'Column_{col}'
                headers.append(col_name)
                column_metadata[col_name] = {"level1": col_name}
            
            headers = self._handle_duplicate_names(headers)
            # æ›´æ–°å…ƒæ•°æ®çš„key
            column_metadata = {h: {"level1": h} for h in headers}
            return headers, column_metadata
        
        else:
            # å¤šè¡¨å¤´ï¼šå±•å¹³
            column_headers = []
            for col in cols_to_process:
                parts = []
                levels = {}
                for row_idx, row in enumerate(range(header_start, header_end + 1), 1):
                    value = self.get_cell_value(row, col)
                    if value is not None:
                        part = str(value).strip()
                        parts.append(part)
                        levels[f"level{row_idx}"] = part
                
                # å»é‡è¿ç»­ç›¸åŒå€¼
                unique_parts = []
                for p in parts:
                    if not unique_parts or p != unique_parts[-1]:
                        unique_parts.append(p)
                
                col_name = '_'.join(unique_parts) if unique_parts else f'Column_{col}'
                column_headers.append(col_name)
                column_metadata[col_name] = levels
            
            column_headers = self._handle_duplicate_names(column_headers)
            
            # é‡æ–°æ˜ å°„å…ƒæ•°æ®
            new_metadata = {}
            for i, header in enumerate(column_headers):
                original_name = '_'.join(unique_parts) if (unique_parts := list(column_metadata.values())[i].values()) else f'Column_{i+1}'
                new_metadata[header] = list(column_metadata.values())[i]
            
            return column_headers, new_metadata
    
    def _handle_duplicate_names(self, names: List[str]) -> List[str]:
        """å¤„ç†é‡å¤åˆ—å"""
        counts = defaultdict(int)
        result = []
        for name in names:
            if counts[name] > 0:
                result.append(f"{name}_{counts[name]}")
            else:
                result.append(name)
            counts[name] += 1
        return result
    
    def to_dataframe(self, analysis: HeaderAnalysis = None,
                    llm_api_key: Optional[str] = None,
                    llm_base_url: Optional[str] = None,
                    llm_model: Optional[str] = None,
                    preview_max_rows: Optional[int] = None,
                    preview_max_cols: Optional[int] = None) -> Tuple[pd.DataFrame, HeaderAnalysis, Dict[str, Dict]]:
        """
        è½¬æ¢ä¸ºDataFrame
        
        å‚æ•°:
            analysis: é¢„å…ˆçš„åˆ†æç»“æœï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨LLMè‡ªåŠ¨åˆ†æï¼ˆå¿…é€‰ï¼‰
            llm_api_key: LLM APIå¯†é’¥ï¼ˆå¿…å¡«ï¼Œå¦‚æœanalysisä¸ºNoneï¼‰
            llm_base_url: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼‰
            llm_model: LLMæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
            preview_max_rows: é¢„è§ˆæœ€å¤§è¡Œæ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
            preview_max_cols: é¢„è§ˆæœ€å¤§åˆ—æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
        
        è¿”å›:
            (DataFrame, åˆ†æç»“æœ, åˆ—ç»“æ„å…ƒæ•°æ®)
        """
        if analysis is None:
            # ä½¿ç”¨LLMè¿›è¡Œåˆ†æï¼ˆåŒ…å«è¡Œæ£€æµ‹å’Œåˆ—æ£€æµ‹ï¼‰
            logger.info("ğŸ¤– ä½¿ç”¨LLMè¿›è¡Œè¡¨å¤´åˆ†æï¼ˆåŒ…å«è¡Œæ£€æµ‹å’Œåˆ—æ£€æµ‹ï¼‰...")
            analysis = self.analyze_with_llm(
                llm_api_key, 
                llm_base_url, 
                llm_model,
                preview_max_rows=preview_max_rows,
                preview_max_cols=preview_max_cols
            )
            logger.info("âœ… LLMåˆ†æå®Œæˆ")
        
        headers, column_metadata = self.extract_headers(analysis)
        
        # ç¡®å®šè¦è¯»å–çš„åˆ—ï¼ˆå¦‚æœæŒ‡å®šäº†æœ‰æ•ˆåˆ—ï¼Œåªè¯»å–æœ‰æ•ˆåˆ—ï¼‰
        cols_to_read = analysis.valid_cols if analysis.valid_cols is not None else list(range(1, self.ws.max_column + 1))
        
        logger.info(f"ğŸ“Š è¯»å–æ•°æ®: ä» {len(cols_to_read)} åˆ—è¯»å–æ•°æ®")
        
        # è¯»å–æ•°æ®
        data = []
        for row in range(analysis.data_start_row, self.ws.max_row + 1):
            row_data = []
            for col in cols_to_read:
                row_data.append(self.ws.cell(row, col).value)
            if any(v is not None for v in row_data):
                data.append(row_data)
        
        df = pd.DataFrame(data, columns=headers)
        logger.info(f"âœ… DataFrame åˆ›å»ºå®Œæˆ: {len(df)} è¡Œ x {len(df.columns)} åˆ—")
        return df, analysis, column_metadata
    
    def close(self):
        """å…³é—­å·¥ä½œç°¿"""
        try:
            self.wb.close()
        except Exception:
            pass


def process_excel_file(
    filepath: str,
    output_dir: str,
    sheet_name: str = None,
    output_filename: str = None,
    llm_api_key: Optional[str] = None,
    llm_base_url: Optional[str] = None,
    llm_model: Optional[str] = None,
    preview_max_rows: Optional[int] = None,
    preview_max_cols: Optional[int] = None
) -> ExcelProcessResult:
    """
    å¤„ç†Excelæ–‡ä»¶çš„ä¸»å‡½æ•°
    
    å‚æ•°:
        filepath: Excelæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        sheet_name: å·¥ä½œè¡¨åç§°
        output_filename: è¾“å‡ºæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        llm_api_key: LLM APIå¯†é’¥ï¼ˆå¿…å¡«ï¼‰
        llm_base_url: LLM APIåœ°å€ï¼ˆå¯é€‰ï¼‰
        llm_model: LLMæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
        preview_max_rows: é¢„è§ˆæœ€å¤§è¡Œæ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
        preview_max_cols: é¢„è§ˆæœ€å¤§åˆ—æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
    
    è¿”å›:
        ExcelProcessResult
    """
    try:
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        # å¤„ç†Excelï¼ˆä½¿ç”¨LLMè¿›è¡Œåˆ†æï¼ŒåŒ…å«è¡Œæ£€æµ‹å’Œåˆ—æ£€æµ‹ï¼‰
        processor = SmartHeaderProcessor(filepath, sheet_name)
        df, analysis, column_metadata = processor.to_dataframe(
            llm_api_key=llm_api_key,
            llm_base_url=llm_base_url,
            llm_model=llm_model,
            preview_max_rows=preview_max_rows,
            preview_max_cols=preview_max_cols
        )
        processor.close()
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        if not output_filename:
            base_name = Path(filepath).stem
            output_filename = f"{base_name}_processed"
        
        # ä¿å­˜CSV
        csv_path = os.path.join(output_dir, f"{output_filename}.csv")
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        # æå–å­—æ®µå€¼æ ·æœ¬ï¼ˆåˆ†ç»„èšåˆåçš„å¸¸è§å€¼ï¼‰
        logger.info("ğŸ“Š æå–å­—æ®µå€¼æ ·æœ¬...")
        column_value_samples = extract_column_value_samples(df, max_samples_per_column=10)
        
        # å°†å€¼æ ·æœ¬ä¿¡æ¯åˆå¹¶åˆ°åˆ—å…ƒæ•°æ®ä¸­
        for col_name, samples in column_value_samples.items():
            if col_name in column_metadata:
                column_metadata[col_name]["value_samples"] = samples
            else:
                # å¦‚æœåˆ—ä¸åœ¨å…ƒæ•°æ®ä¸­ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰ï¼Œåˆ›å»ºæ–°çš„å…ƒæ•°æ®é¡¹
                column_metadata[col_name] = {"value_samples": samples}
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            "header_analysis": analysis.to_dict(),
            "column_metadata": column_metadata,
            "column_names": list(df.columns),
            "row_count": len(df),
            "original_file": os.path.basename(filepath)
        }
        metadata_path = os.path.join(output_dir, f"{output_filename}_metadata.json")
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°å¤„ç†åçš„JSONå…ƒæ•°æ®
        logger.info("=" * 80)
        logger.info("ğŸ“„ å¤„ç†åçš„JSONå…ƒæ•°æ®:")
        logger.info("=" * 80)
        logger.info(json.dumps(metadata, ensure_ascii=False, indent=2))
        logger.info("=" * 80)
        
        return ExcelProcessResult(
            success=True,
            header_analysis=analysis,
            processed_file_path=csv_path,
            metadata_file_path=metadata_path,
            column_names=list(df.columns),
            column_metadata=column_metadata,
            row_count=len(df),
            error_message=None
        )
        
    except Exception as e:
        import traceback
        error_msg = f"{str(e)}\n{traceback.format_exc()}"
        return ExcelProcessResult(
            success=False,
            header_analysis=None,
            processed_file_path=None,
            metadata_file_path=None,
            column_names=[],
            column_metadata={},
            row_count=0,
            error_message=error_msg
        )


def get_sheet_names(filepath: str) -> List[str]:
    """è·å–Excelæ–‡ä»¶çš„æ‰€æœ‰å·¥ä½œè¡¨åç§°"""
    try:
        wb = load_workbook(filepath, read_only=True)
        sheets = wb.sheetnames
        wb.close()
        return sheets
    except Exception as e:
        return []


def extract_column_value_samples(
    df: pd.DataFrame,
    max_samples_per_column: int = 10,
    max_unique_ratio: float = 0.5
) -> Dict[str, Dict[str, Any]]:
    """
    æå–æ¯ä¸ªå­—æ®µçš„å¸¸è§å€¼æ ·æœ¬ï¼ˆé€šè¿‡åˆ†ç»„èšåˆï¼‰
    
    å‚æ•°:
        df: æ•°æ®æ¡†
        max_samples_per_column: æ¯ä¸ªå­—æ®µæœ€å¤šä¿ç•™çš„æ ·æœ¬æ•°é‡
        max_unique_ratio: å¦‚æœå”¯ä¸€å€¼å æ¯”è¶…è¿‡æ­¤æ¯”ä¾‹ï¼Œåˆ™åªæä¾›ç»Ÿè®¡ä¿¡æ¯è€Œä¸ç»Ÿè®¡é¢‘ç‡
    
    è¿”å›:
        å­—å…¸ï¼Œkeyä¸ºåˆ—åï¼Œvalueä¸ºåŒ…å«å¸¸è§å€¼å’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
    """
    column_samples = {}
    
    for col_name in df.columns:
        col_data = df[col_name]
        
        # è·³è¿‡å®Œå…¨ä¸ºç©ºçš„åˆ—
        if col_data.isna().all():
            continue
        
        # è®¡ç®—éç©ºå€¼æ•°é‡
        non_null_count = col_data.notna().sum()
        if non_null_count == 0:
            continue
        
        # è®¡ç®—å”¯ä¸€å€¼æ•°é‡
        unique_count = col_data.nunique()
        unique_ratio = unique_count / non_null_count if non_null_count > 0 else 1.0
        
        sample_info = {
            "total_count": len(col_data),
            "non_null_count": int(non_null_count),
            "null_count": int(col_data.isna().sum()),
            "unique_count": int(unique_count),
            "data_type": str(col_data.dtype)
        }
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºæ•°å€¼ç±»å‹
        is_numeric = pd.api.types.is_numeric_dtype(col_data)
        
        if is_numeric:
            # æ•°å€¼ç±»å‹ï¼šæä¾›ç»Ÿè®¡ä¿¡æ¯å’Œå¸¸è§å€¼ï¼ˆå¦‚æœå”¯ä¸€å€¼ä¸å¤ªå¤šï¼‰
            sample_info["is_numeric"] = True
            non_null_data = col_data.dropna()
            if len(non_null_data) > 0:
                sample_info["min"] = float(non_null_data.min())
                sample_info["max"] = float(non_null_data.max())
                sample_info["mean"] = float(non_null_data.mean())
                sample_info["median"] = float(non_null_data.median())
            else:
                sample_info["min"] = None
                sample_info["max"] = None
                sample_info["mean"] = None
                sample_info["median"] = None
            
            # å¦‚æœå”¯ä¸€å€¼ä¸å¤ªå¤šï¼Œä¹Ÿç»Ÿè®¡é¢‘ç‡
            if unique_ratio <= max_unique_ratio and unique_count <= 100:
                value_counts = col_data.value_counts().head(max_samples_per_column)
                sample_info["top_values"] = [
                    {"value": float(k) if pd.notna(k) else None, "count": int(v)}
                    for k, v in value_counts.items()
                ]
            elif unique_count <= max_samples_per_column:
                # å³ä½¿å”¯ä¸€å€¼æ¯”ä¾‹é«˜ï¼Œä½†å¦‚æœæ€»æ•°ä¸å¤šï¼Œä¹Ÿå±•ç¤ºæ‰€æœ‰å€¼
                value_counts = col_data.value_counts().head(max_samples_per_column)
                sample_info["top_values"] = [
                    {"value": float(k) if pd.notna(k) else None, "count": int(v)}
                    for k, v in value_counts.items()
                ]
                sample_info["note"] = f"å”¯ä¸€å€¼è¾ƒå¤šï¼ˆ{unique_count}ä¸ªï¼‰ï¼Œå±•ç¤ºæ‰€æœ‰å€¼"
        else:
            # éæ•°å€¼ç±»å‹ï¼šç»Ÿè®¡é¢‘ç‡
            sample_info["is_numeric"] = False
            
            # å¦‚æœå”¯ä¸€å€¼å¤ªå¤šï¼Œåªæä¾›ç»Ÿè®¡ä¿¡æ¯
            if unique_ratio > max_unique_ratio:
                sample_info["note"] = f"å”¯ä¸€å€¼è¾ƒå¤šï¼ˆ{unique_count}ä¸ªï¼‰ï¼Œä»…å±•ç¤ºéƒ¨åˆ†å¸¸è§å€¼"
                # ä»ç„¶å±•ç¤ºå‰Nä¸ªæœ€å¸¸è§çš„å€¼
                value_counts = col_data.value_counts().head(max_samples_per_column)
                sample_info["top_values"] = [
                    {"value": str(k) if pd.notna(k) else "ç©ºå€¼", "count": int(v)}
                    for k, v in value_counts.items()
                ]
            else:
                # å”¯ä¸€å€¼ä¸å¤ªå¤šï¼Œç»Ÿè®¡æ‰€æœ‰å€¼çš„é¢‘ç‡
                value_counts = col_data.value_counts().head(max_samples_per_column)
                sample_info["top_values"] = [
                    {"value": str(k) if pd.notna(k) else "ç©ºå€¼", "count": int(v)}
                    for k, v in value_counts.items()
                ]
        
        column_samples[col_name] = sample_info
    
    return column_samples


def _build_column_hierarchy_tree(column_metadata: Dict[str, Dict]) -> str:
    """
    æ„å»ºåˆ—å±‚çº§ç»“æ„çš„æ ‘å½¢å±•ç¤º
    
    å‚æ•°:
        column_metadata: åˆ—å…ƒæ•°æ®å­—å…¸
    
    è¿”å›:
        æ ¼å¼åŒ–çš„æ ‘å½¢ç»“æ„å­—ç¬¦ä¸²
    """
    if not column_metadata:
        return ""
    
    # æ„å»ºæ ‘å½¢ç»“æ„
    tree = {}
    
    for col_name, meta in column_metadata.items():
        # è·å–æ‰€æœ‰å±‚çº§
        levels = []
        level_keys = sorted([k for k in meta.keys() if k.startswith('level')], 
                          key=lambda x: int(x.replace('level', '')))
        for level_key in level_keys:
            value = meta.get(level_key)
            if value and str(value).strip():
                levels.append(str(value).strip())
        
        # å¦‚æœæ²¡æœ‰å±‚çº§ä¿¡æ¯ï¼Œä½¿ç”¨åˆ—åæœ¬èº«
        if not levels:
            levels = [col_name]
        
        # æ„å»ºæ ‘
        current = tree
        for i, level_value in enumerate(levels):
            if level_value not in current:
                current[level_value] = {}
            current = current[level_value]
    
    # é€’å½’ç”Ÿæˆæ ‘å½¢å­—ç¬¦ä¸²
    def _format_tree(node: Dict, prefix: str = "", is_last: bool = True, depth: int = 0) -> List[str]:
        lines = []
        items = list(node.items())
        
        for idx, (key, children) in enumerate(items):
            is_last_item = (idx == len(items) - 1)
            current_prefix = "â””â”€ " if is_last_item else "â”œâ”€ "
            
            if children:
                # æœ‰å­èŠ‚ç‚¹
                lines.append(f"{prefix}{current_prefix}{key}")
                next_prefix = prefix + ("   " if is_last_item else "â”‚  ")
                child_lines = _format_tree(children, next_prefix, is_last_item, depth + 1)
                lines.extend(child_lines)
            else:
                # å¶å­èŠ‚ç‚¹
                lines.append(f"{prefix}{current_prefix}{key}")
        
        return lines
    
    tree_lines = _format_tree(tree)
    return "\n".join(tree_lines)


def generate_analysis_prompt(
    process_result: ExcelProcessResult,
    custom_prompt: str = None,
    include_metadata: bool = True
) -> str:
    """
    æ ¹æ®Excelå¤„ç†ç»“æœç”Ÿæˆæ•°æ®åˆ†ææç¤ºè¯
    
    å‚æ•°:
        process_result: Excelå¤„ç†ç»“æœ
        custom_prompt: è‡ªå®šä¹‰åˆ†ææç¤ºè¯
        include_metadata: æ˜¯å¦åŒ…å«åˆ—ç»“æ„å…ƒæ•°æ®
    
    è¿”å›:
        æ ¼å¼åŒ–çš„æç¤ºè¯
    """
    if not process_result.success:
        return ""
    
    # åŸºç¡€ä¿¡æ¯
    prompt_parts = []
    
    # æ·»åŠ è¯­è¨€è¦æ±‚ï¼ˆå¿…é¡»åœ¨æœ€å‰é¢ï¼‰
    prompt_parts.append("**é‡è¦è¦æ±‚ï¼šè¯·ä½¿ç”¨ä¸­æ–‡è¿›è¡Œæ‰€æœ‰åˆ†æå’Œå›ç­”ï¼ŒåŒ…æ‹¬ä»£ç æ³¨é‡Šã€åˆ†ææŠ¥å‘Šç­‰æ‰€æœ‰å†…å®¹ã€‚**")
    prompt_parts.append("")
    prompt_parts.append("**ç¦æ­¢è¦æ±‚ï¼šè¯·ä¸è¦ç”Ÿæˆä»»ä½•å›¾è¡¨ç»˜åˆ¶ä»£ç ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š**")
    prompt_parts.append("- ä¸è¦ä½¿ç”¨ matplotlibã€plotlyã€seaborn ç­‰ç»˜å›¾åº“")
    prompt_parts.append("- ä¸è¦ä½¿ç”¨ plt.figure()ã€plt.plot()ã€plt.savefig() ç­‰ç»˜å›¾å‡½æ•°")
    prompt_parts.append("- ä¸è¦ä½¿ç”¨ .plot()ã€.hist() ç­‰ pandas ç»˜å›¾æ–¹æ³•")
    prompt_parts.append("- ä¸è¦ä¿å­˜ä»»ä½•å›¾ç‰‡æ–‡ä»¶ï¼ˆ.pngã€.jpgã€.svg ç­‰ï¼‰")
    prompt_parts.append("**è¯·ä¸“æ³¨äºæ•°æ®åˆ†æå’Œç»Ÿè®¡è®¡ç®—ï¼Œä¸è¦ç”Ÿæˆå¯è§†åŒ–ä»£ç ã€‚**")
    prompt_parts.append("")
    
    if custom_prompt:
        prompt_parts.append(custom_prompt)
    else:
        prompt_parts.append("è¯·å¯¹ä¸Šä¼ çš„æ•°æ®è¿›è¡Œå…¨é¢åˆ†æï¼Œç”Ÿæˆæ•°æ®åˆ†ææŠ¥å‘Šã€‚")
    
    # æ·»åŠ æ•°æ®æ–‡ä»¶ä¿¡æ¯ï¼ˆé‡è¦ï¼šå‘Šè¯‰AIéœ€è¦è¯»å–CSVæ–‡ä»¶ï¼‰
    if process_result.processed_file_path:
        csv_filename = os.path.basename(process_result.processed_file_path)
        prompt_parts.append(f"\n\n## æ•°æ®æ–‡ä»¶")
        prompt_parts.append(f"**é‡è¦ï¼šå·¥ä½œç©ºé—´ä¸­å·²å‡†å¤‡å¥½å¤„ç†åçš„CSVæ•°æ®æ–‡ä»¶ï¼Œæ–‡ä»¶åä¸ºï¼š`{csv_filename}`**")
        prompt_parts.append(f"")
        prompt_parts.append(f"**è¯·åŠ¡å¿…ä½¿ç”¨ä»¥ä¸‹ä»£ç è¯»å–æ•°æ®æ–‡ä»¶è¿›è¡Œåˆ†æï¼š**")
        prompt_parts.append(f"```python")
        prompt_parts.append(f"import pandas as pd")
        prompt_parts.append(f"")
        prompt_parts.append(f"# è¯»å–å¤„ç†åçš„CSVæ–‡ä»¶")
        prompt_parts.append(f"df = pd.read_csv('{csv_filename}')")
        prompt_parts.append(f"print(f'æ•°æ®å½¢çŠ¶: {{df.shape}}')")
        prompt_parts.append(f"print(f'åˆ—å: {{list(df.columns)}}')")
        prompt_parts.append(f"```")
        prompt_parts.append(f"")
        prompt_parts.append(f"**æ³¨æ„ï¼š**")
        prompt_parts.append(f"- CSVæ–‡ä»¶å·²ä¿å­˜åœ¨å½“å‰å·¥ä½œç©ºé—´ç›®å½•ä¸­")
        prompt_parts.append(f"- è¯·ä½¿ç”¨ `pd.read_csv('{csv_filename}')` è¯»å–æ•°æ®")
        prompt_parts.append(f"- ä¸è¦ä»…æ ¹æ®å…ƒæ•°æ®è¿›è¡Œåˆ†æï¼Œå¿…é¡»è¯»å–å®é™…æ•°æ®æ–‡ä»¶è¿›è¡Œè®¡ç®—")
        prompt_parts.append(f"")
    
    # æ·»åŠ æ•°æ®æ¦‚å†µ
    prompt_parts.append(f"\n## æ•°æ®æ¦‚å†µ")
    prompt_parts.append(f"- æ•°æ®è¡Œæ•°: {process_result.row_count}")
    prompt_parts.append(f"- åˆ—æ•°: {len(process_result.column_names)}")
    
    # æ·»åŠ è¡¨å¤´ç±»å‹ä¿¡æ¯ï¼ˆä»…ä¿ç•™å¯¹åˆ†ææœ‰ç”¨çš„ä¿¡æ¯ï¼‰
    if process_result.header_analysis:
        ha = process_result.header_analysis
        if ha.header_type == 'multi':
            prompt_parts.append(f"\n## è¡¨å¤´ç»“æ„")
            prompt_parts.append(f"- è¡¨å¤´ç±»å‹: å¤šçº§è¡¨å¤´ï¼ˆ{ha.header_rows}å±‚ï¼‰")
    
    # æ·»åŠ åˆ—ç»“æ„å…ƒæ•°æ®ï¼ˆå¸®åŠ©AIç†è§£åˆ—ä¹‹é—´çš„å…³ç³»ï¼‰
    if include_metadata and process_result.column_metadata:
        # æ£€æŸ¥æ˜¯å¦æœ‰å¤šçº§ç»“æ„
        has_multi_level = any(
            len(meta) > 1 
            for meta in process_result.column_metadata.values()
        )
        
        if has_multi_level:
            prompt_parts.append(f"\n## åˆ—å±‚çº§ç»“æ„ï¼ˆå¤šçº§è¡¨å¤´è¯­ä¹‰å…³ç³»ï¼‰")
            prompt_parts.append("ä»¥ä¸‹æ ‘å½¢ç»“æ„å±•ç¤ºäº†åˆ—ä¹‹é—´çš„å±‚çº§åˆ†ç»„å…³ç³»ï¼Œæœ‰åŠ©äºç†è§£æ•°æ®çš„ä¸šåŠ¡å«ä¹‰ï¼š")
            prompt_parts.append("")
            hierarchy_tree = _build_column_hierarchy_tree(process_result.column_metadata)
            if hierarchy_tree:
                prompt_parts.append(hierarchy_tree)
            else:
                # å¦‚æœæ ‘å½¢æ„å»ºå¤±è´¥ï¼Œä½¿ç”¨åˆ†ç»„å±•ç¤º
                groups = defaultdict(list)
                for col_name, meta in process_result.column_metadata.items():
                    level1 = meta.get('level1', col_name)
                    groups[level1].append(col_name)
                
                for group, cols in groups.items():
                    if len(cols) > 1:
                        prompt_parts.append(f"- {group}: {', '.join(cols)}")
    
    # æ·»åŠ å®Œæ•´çš„åˆ—ååˆ—è¡¨
    prompt_parts.append(f"\n## å®Œæ•´åˆ—ååˆ—è¡¨")
    if len(process_result.column_names) <= 30:
        # å¦‚æœåˆ—æ•°ä¸å¤šï¼Œå…¨éƒ¨å±•ç¤º
        for idx, col_name in enumerate(process_result.column_names, 1):
            prompt_parts.append(f"{idx}. {col_name}")
    else:
        # å¦‚æœåˆ—æ•°å¾ˆå¤šï¼Œå±•ç¤ºå‰20ä¸ªå’Œå10ä¸ª
        for idx, col_name in enumerate(process_result.column_names[:20], 1):
            prompt_parts.append(f"{idx}. {col_name}")
        prompt_parts.append(f"... (çœç•¥ä¸­é—´ {len(process_result.column_names) - 30} åˆ—) ...")
        for idx, col_name in enumerate(process_result.column_names[-10:], len(process_result.column_names) - 9):
            prompt_parts.append(f"{idx}. {col_name}")
        prompt_parts.append(f"\n(å…± {len(process_result.column_names)} åˆ—)")
    
    # æ·»åŠ å­—æ®µå€¼æ ·æœ¬ä¿¡æ¯ï¼ˆä»¥JSONæ ¼å¼æä¾›ï¼Œæ›´ç»“æ„åŒ–ï¼‰
    if include_metadata and process_result.column_metadata:
        prompt_parts.append(f"\n## å­—æ®µå€¼æ ·æœ¬ï¼ˆå¸¸è§å€¼ç»Ÿè®¡ï¼‰")
        prompt_parts.append("ä»¥ä¸‹JSONæ ¼å¼å±•ç¤ºäº†æ¯ä¸ªå­—æ®µçš„å¸¸è§å€¼åŠå…¶å‡ºç°é¢‘ç‡ï¼Œæœ‰åŠ©äºç†è§£æ•°æ®çš„å®é™…å†…å®¹ï¼š")
        prompt_parts.append("")
        
        # æ„å»ºåŒ…å«å€¼æ ·æœ¬çš„column_metadata JSON
        column_metadata_with_samples = {}
        for col_name in process_result.column_names:
            if col_name in process_result.column_metadata:
                column_metadata_with_samples[col_name] = process_result.column_metadata[col_name]
        
        # å°†column_metadataè½¬æ¢ä¸ºæ ¼å¼åŒ–çš„JSONå­—ç¬¦ä¸²
        prompt_parts.append("```json")
        prompt_parts.append(json.dumps(column_metadata_with_samples, ensure_ascii=False, indent=2))
        prompt_parts.append("```")
        prompt_parts.append("")
        
        prompt_parts.append("**è¯´æ˜ï¼š**")
        prompt_parts.append("- æ¯ä¸ªå­—æ®µçš„å…ƒæ•°æ®åŒ…å« `value_samples` å­—æ®µï¼Œå…¶ä¸­åŒ…å«è¯¥å­—æ®µçš„ç»Ÿè®¡ä¿¡æ¯å’Œå¸¸è§å€¼")
        prompt_parts.append("- `value_samples.top_values` æ•°ç»„å±•ç¤ºäº†å‡ºç°é¢‘ç‡æœ€é«˜çš„å€¼åŠå…¶å‡ºç°æ¬¡æ•°")
        prompt_parts.append("- å¯¹äºæ•°å€¼ç±»å‹å­—æ®µï¼Œè¿˜åŒ…å« `min`ã€`max`ã€`mean`ã€`median` ç­‰ç»Ÿè®¡ä¿¡æ¯")
    
    # åœ¨æœ«å°¾å†æ¬¡å¼ºè°ƒè¦æ±‚
    prompt_parts.append("\n\n**å†æ¬¡æé†’ï¼šè¯·åŠ¡å¿…ä½¿ç”¨ä¸­æ–‡è¿›è¡Œæ‰€æœ‰åˆ†æã€ä»£ç æ³¨é‡Šå’ŒæŠ¥å‘Šæ’°å†™ï¼Œä¸”ä¸è¦ç”Ÿæˆä»»ä½•å›¾è¡¨ç»˜åˆ¶ä»£ç ã€‚**")
    
    full_prompt = '\n'.join(prompt_parts)
    
    # æ‰“å°ç”Ÿæˆçš„æç¤ºè¯
    logger.info("=" * 80)
    logger.info("ğŸ“ ç”Ÿæˆçš„AIåˆ†ææç¤ºè¯:")
    logger.info("=" * 80)
    logger.info(full_prompt)
    logger.info("=" * 80)
    
    return full_prompt

